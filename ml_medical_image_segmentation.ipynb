{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Image Segmentation Model for Medical Diagnosis using CNN, PyTorch and OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In healthcare and medical science, the fusion of artificial intelligence and deep learning is revolutionizing diagnostics. My project focuses on the precise segmentation of polyps from colonoscopy imagesâ€”a vital tool for medical practitioners.\n",
    "\n",
    "In this project, we aim to develop an image segmentation model using CNN and deploy it with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://github.com/albumentations-team/albumentations/tree/main/albumentations\n",
    "from albumentations import Resize\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose, OneOf\n",
    "from albumentations.augmentations.geometric.rotate import RandomRotate90\n",
    "\n",
    "from ml_deploy.network import UNetPP\n",
    "from ml_deploy.dataset import DataSet\n",
    "from ml_deploy.train import train\n",
    "from ml_deploy.validate import validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the image dataset and create train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, transform=None):\n",
    "        self.img_ids = img_ids\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_ext = img_ext\n",
    "        self.mask_ext = mask_ext\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "\n",
    "        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))\n",
    "\n",
    "        mask = []\n",
    "        mask.append(cv2.imread(os.path.join(self.mask_dir,\n",
    "                                            img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])\n",
    "        mask = np.dstack(mask)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        img = img.astype('float32') / 255\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        mask = mask.astype('float32') / 255\n",
    "        mask = mask.transpose(2, 0, 1)\n",
    "\n",
    "        return img, mask, {'img_id': img_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up config.\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "extn = config[\"extn\"]\n",
    "epochs = config[\"epochs\"]\n",
    "log_path = config[\"log_path\"]\n",
    "mask_path = config[\"mask_path\"]\n",
    "image_path = config[\"image_path\"]\n",
    "model_path = config[\"model_path\"]\n",
    "im_width = config[\"im_width\"]\n",
    "im_height = config[\"im_height\"]\n",
    "model_path = config[\"model_path\"]\n",
    "output_path = config[\"output_path\"]\n",
    "best_iou, trigger = 0, 0\n",
    "\n",
    "\n",
    "\n",
    "## split images into train and validation datasets\n",
    "extn_ = f\"*{extn}\"\n",
    "img_ids = glob(os.path.join(image_path, extn_))\n",
    "img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n",
    "train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "## define data transformations using the albumentations library\n",
    "train_transform = Compose([\n",
    "    RandomRotate90(),\n",
    "    # transforms.Flip(),\n",
    "    OneOf([\n",
    "        transforms.HueSaturationValue(),\n",
    "        transforms.RandomBrightness(),\n",
    "        transforms.RandomContrast(),\n",
    "    ], p=1),\n",
    "    # Resize(256, 256),\n",
    "    transforms.Normalize(),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    # Resize(256, 256),\n",
    "    transforms.Normalize(),\n",
    "])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    img = imread(image_name)\n",
    "    img = val_transform(image=img)[\"image\"]\n",
    "    img = img.astype('float32') / 255\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    return img\n",
    "\n",
    "train_dataset = DataSet(\n",
    "    img_ids=train_img_ids,\n",
    "    img_dir=image_path,\n",
    "    mask_dir=mask_path,\n",
    "    img_ext=extn,\n",
    "    mask_ext=extn,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = DataSet(\n",
    "    img_ids=val_img_ids,\n",
    "    img_dir=image_path,\n",
    "    mask_dir=mask_path,\n",
    "    img_ext=extn,\n",
    "    mask_ext=extn,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "\n",
    "# create train and validation data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model object and train with epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetPP(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.deep_supervision = deep_supervision\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n",
    "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
    "        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n",
    "\n",
    "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n",
    "\n",
    "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "\n",
    "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "\n",
    "        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x0_0 = self.conv0_0(input)\n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
    "\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
    "\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
    "\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            output1 = self.final1(x0_1)\n",
    "            output2 = self.final2(x0_2)\n",
    "            output3 = self.final3(x0_3)\n",
    "            output4 = self.final4(x0_4)\n",
    "            return [output1, output2, output3, output4]\n",
    "\n",
    "        else:\n",
    "            output = self.final(x0_4)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetPP(1, 3, True)\n",
    "\n",
    "# port moel to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(params, lr=1e-3, weight_decay=1e-4)\n",
    "log = OrderedDict([\n",
    "    ('epoch', []),\n",
    "    ('loss', []),\n",
    "    ('iou', []),\n",
    "    ('val_loss', []),\n",
    "    ('val_iou', []),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# run train loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch [{epoch}/{epochs}]')\n",
    "\n",
    "    # train for one epoch\n",
    "    train_log = train(True, train_loader, model, criterion, optimizer)\n",
    "    \n",
    "    # evaluate on validation set\n",
    "    val_log = validate(True, val_loader, model, criterion)\n",
    "\n",
    "    print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
    "                % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
    "\n",
    "    log['epoch'].append(epoch)\n",
    "    log['loss'].append(train_log['loss'])\n",
    "    log['iou'].append(train_log['iou'])\n",
    "    log['val_loss'].append(val_log['loss'])\n",
    "    log['val_iou'].append(val_log['iou'])\n",
    "\n",
    "    pd.DataFrame(log).to_csv(log_path, index=False)\n",
    "\n",
    "    trigger += 1\n",
    "\n",
    "    if val_log['iou'] > best_iou:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        best_iou = val_log['iou']\n",
    "        print(\"=> saved best model\")\n",
    "        trigger = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = OrderedDict([\n",
    "    ('epoch', []),\n",
    "    ('loss', []),\n",
    "    ('iou', []),\n",
    "    ('val_loss', []),\n",
    "    ('val_iou', []),\n",
    "])\n",
    "\n",
    "best_iou = 0\n",
    "trigger = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and predict the segmentation using test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--test_img\", default=\"../input/PNG/Original/50.png\", help=\"path to test image\")\n",
    "\n",
    "opt = parser.parse_args()\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "\n",
    "# load pre-trained weights\n",
    "model = UNetPP(1, 3, True)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# port the model to GPU if it is available\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "# set model mode to evaluation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = \"input/PNG/Original/20.png\"\n",
    "image = image_loader(test_img)\n",
    "\n",
    "# convert the image to a batch of 1 image\n",
    "image = np.expand_dims(image,0)\n",
    "\n",
    "# convert numpy array to torch tensor\n",
    "image = torch.from_numpy(image)\n",
    "\n",
    "# port the image to GPU if it is available\n",
    "if torch.cuda.is_available():\n",
    "    image = image.to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = model(image)\n",
    "mask = mask[-1]\n",
    "\n",
    "# convert torch tensor to numpy array, then to a 2-d array, and binary \n",
    "mask = mask.detach().cpu().numpy()\n",
    "mask = np.squeeze(np.squeeze(mask, axis=0), axis=0)\n",
    "mask[mask > -2.5] = 255\n",
    "mask[mask <= -2.5] = 0\n",
    "mask = cv2.resize(mask, (im_width, im_height)) # resize the ouptut image to input image size\n",
    "plt.imshow(mask, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and plot the ground truth mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_mask = \"input/PNG/Ground Truth/115.png\"\n",
    "am = plt.imread(actual_mask)\n",
    "plt.imshow(am, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNET++.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
